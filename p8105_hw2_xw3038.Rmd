---
title: "Homework2"
author: "Avery Wang"
date: 2024-10-01
output: github_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE, message = FALSE)
```

## Problem 1
Load the necessary packages for this assignment 
```{r load_libraries}
library(tidyverse)
library(dplyr)
library(readxl)

```

Read and clean the data; retain line, station, name, station latitude / longitude, routes served, entry, vending, entrance type, and ADA compliance. Convert the entry variable from character (YES vs NO) to a logical variable (the ifelse or case_match function may be useful).

```{r}
#load the data using relative path
NYC_subway=read_csv("./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv",
                    col_types = cols(Route8 =col_character(), 
                                     Route9 = col_character(), 
                                     Route10 =col_character(), 
                                     Route11 = col_character())) |>
  janitor::clean_names() |> 
  select(line:exit_only,vending,ada) |>
  mutate(entry = ifelse(entry == "YES", TRUE, FALSE))
head(NYC_subway,5)
```
The variables include:
Line, station name, station latitude and longtitude, route 1 to 11, entrance type, entry, exit_only and ada information.

The first step is to convert the Route8 to 11 into character type so that all the types of Route are now consistent. Then the `janitor::clean_names()` is used to standardize the column names. And specific columns according to the instruction are selected. Then the entry variable is converted to the logical variable using ifelse. The cleaned dataset now has `r nrow(NYC_subway)` rows and `r ncol(NYC_subway)` columns. And these data are not tidy as we should convert `route` variables from wide to long format. 

```{r}
filter_station=select(NYC_subway,line,station_name,ada,entry,vending)

```
* There are `r nrow(filter_station|>distinct(line, station_name))` distinct stations 

* There are `r nrow(filter_station |> filter(ada == TRUE) |> distinct(line, station_name))` stations that are ADA compliant

```{r}
# Filter for stations without vending machines
without_vending=nrow( filter_station |>
  filter(vending == "NO") )

# Calculate the proportion of entrances without vending machines that allow entry
noentry_without_vending=nrow( filter_station |>
  filter(vending == "NO" & entry=="TRUE") )

```

*The proportion of station entrances / exits without vending allow entrance is `r noentry_without_vending/without_vending`

Reformat data so that route number and route name are distinct variables. 
```{r}
reformat_df=
  NYC_subway |>
  mutate(across(starts_with("route"), as.character)) |>
  pivot_longer(
    route1:route11, 
    names_to = "route_number", 
    values_to = "route_name", 
    values_drop_na = TRUE
    )
```

```{r}
A_station=reformat_df|>filter(route_name == "A")|>distinct(line,station_name)
A_train_ada= reformat_df|>filter(route_name == "A"& ada=="TRUE")|>distinct(line,station_name)
```
* There are `r nrow(A_station)` distinct stations served the A train.
* There are `r nrow(A_train_ada)` distinct A stations that are ADA compliant

## Problem 2
Read and clean the Mr. Trash Wheel Sheet:
Load the excel using `read_excel` and specify the Mr. Trash Wheel sheet and figure out some potential NA cases. Then the `janitor::clean_names()` is used to standardize the column names. The rows without dumspter specific data are dropped the sports ball column is converted to the integer variable using `as.integer`. In order to differentiate the trash wheel type and help prepare the steps for combining, a column named trash_wheel is also added. And the year is converted to integer as well.

The sheet Professor trash wheel and Gwynndan trash wheel are loaded and cleaned similarly
```{r }
#load the Mr. Trash sheet by specifying the excel file
Mr_trash_wheel_sheet=read_excel("./data/Trash_Wheel_Collection_Data.xlsx",
                                sheet = "Mr. Trash Wheel", 
                                na = c("NA", ".", "")) |>
   janitor::clean_names() |> #give reasonable name 
   select(dumpster:homes_powered)|>
   drop_na(dumpster)|> #remove the rows without dumpster specific data
   mutate(sports_balls = as.integer(round(sports_balls)),
          trash_wheel= "Mr. Trash Wheel", 
          year = as.integer(year))

Professor_trash_wheel_sheet=read_excel("./data/Trash_Wheel_Collection_Data.xlsx",
                                sheet = "Professor Trash Wheel",
                                na = c("NA", ".", "")) |>
   janitor::clean_names() |> #give reasonable name 
   select(dumpster:homes_powered)|>
   drop_na(dumpster) |> #remove the rows without dumpster specific data
   mutate(trash_wheel ="Professor trash wheel",
           year = as.integer(year))

Gwynnda_trash_wheel=read_excel("./data/Trash_Wheel_Collection_Data.xlsx",
                                sheet = "Gwynnda Trash Wheel",
                                na = c("NA", ".", "")) |>
   janitor::clean_names() |> #give reasonable name 
   select(dumpster:homes_powered)|>
   drop_na(dumpster) |> #remove the rows without dumpster specific data
   mutate(trash_wheel ="Gwynnda trash wheel",
           year = as.integer(year))


```

* Mr. trash wheel sheet has `r nrow(Mr_trash_wheel_sheet)` rows and `r ncol(Mr_trash_wheel_sheet)` columns. With the variables: `r names(Mr_trash_wheel_sheet)`

* Professor trash wheel sheet has `r nrow(Professor_trash_wheel_sheet)` rows and `r ncol(Professor_trash_wheel_sheet)` columns. With the variables: `r names(Professor_trash_wheel_sheet)`

* Gwynnda trash wheel sheet has `r nrow(Gwynnda_trash_wheel)` rows and `r ncol(Gwynnda_trash_wheel)` columns. With the variables: `r names(Gwynnda_trash_wheel)`

Then combine the dataset with `bind_rows` :
```{r}
# Combine all three datasets into one tidy dataset
combined_trash_wheel_data <- bind_rows(Mr_trash_wheel_sheet, Professor_trash_wheel_sheet, Gwynnda_trash_wheel)
head(combined_trash_wheel_data,5)
  
```
```{r}
#filter by trash_wheel
weight_Professor=combined_trash_wheel_data |> 
  select(year,month,weight_tons,trash_wheel)|>
  filter(trash_wheel=="Professor trash wheel")

#filter by year and month for the Gwynnda trash wheel
cigg_gwy=combined_trash_wheel_data |> 
  select(year,month,cigarette_butts,trash_wheel)|>
  filter(trash_wheel=="Gwynnda trash wheel", year==2022, month=="June")

```

The new dataset has `r nrow(combined_trash_wheel_data)` observations and `r ncol(combined_trash_wheel_data)` columns. With key variables: `r names(combined_trash_wheel_data)`. 

* The total trash weight collected by Professor Trash Wheel is `r sum(weight_Professor$weight_tons, na.rm = TRUE)` tons.

* The total number of cigarette butts collected by Gwynnda in June of 2022 is `r format(sum(cigg_gwy$cigarette_butts,na.rm = TRUE))`

## Problem 3
Import, clean and organize 
```{r}
individual_bake <- read_csv("./data/gbb_datasets/bakers.csv", na = c("NA", "N/A", "UNKNOWN", "Unknown", "")) |>
  janitor::clean_names() |>
  mutate(baker_first_name = sapply(strsplit(baker_name, " "), `[`, 1)) |> # Extract the first word as first name
  select(baker_first_name,everything())

their_bake= read_csv("./data/gbb_datasets/bakes.csv", na = c("NA", "N/A", "UNKNOWN", "Unknown", "")) |>
  janitor::clean_names() |>
  mutate(baker = ifelse(baker == "\"Jo\"", "Jo", baker)) |>
  select(baker,everything())|> #it is using the first name here 
  rename(baker_first_name=baker)

results = read_csv("./data/gbb_datasets/results.csv", skip=2)|> 
  janitor::clean_names() |> 
  select(baker, everything())|>
  rename(baker_first_name=baker)|>
  drop_na(result)|>
  mutate(
    result = case_match(
      result, 
      "IN" ~ "stayed in",
      "OUT" ~ "Eliminated",
      "STAR BAKER" ~ "Star Baker",
      "WINNER" ~ "Series Winner",
      "Runner-up" ~ "Series Runner up",
      "WD" ~ "withdrew"
    )
  )

```
For each dataset `bakers.csv, bakes.csv, and results.csv` the `read_csv()` function reads the CSV files. Special values like "NA", "N/A", "UNKNOWN", "Unknown", and empty strings are treated as missing (NA). `janitor::clean_names()` is applied to each dataset to standardize the columns.

In individual_bake, the `mutate()` function uses `strsplit()` and `sapply()` to extract the first name from the baker_name column. This helps prepare the joining step with their_bake as only first name of the baker exist in that dataset. The first name is stored in a new column baker_first_name. 

In their_bake, the `mutate()` function is used to clean up inconsistencies in the baker column. Specifically, if a baker's name is "\"Jo\"", it is replaced with "Jo" to standardize the name. The baker column is renamed to baker_first_name which helps the joining step with the other two datasets.

In results dataframe, the first two columns are skipped since the information starts from row 3. The column baker is also renamed to baker_first_name, and the column result with missing values are dropped. `case_match()` is also applied to clean and standardize values in the result column.

To check for completeness and correctness across datasets, `anti_join` is used. The anti_join of their_bake and individual_bake shows that all the bakers in their_bake can find corresponding personal information in the individual_bake dataframe. 
```{r}
anti_join(their_bake, individual_bake)
```
And the result of `anti_join(results,their_bake)` shows that in series 2, the person Jo cannot find the corresponding results in all the episodes she attended. But since Joanne is the winner of series 2, Jo actually might be Joanne. Correspondingly, I changed the first name Joanne to Jo in the result dataframe 
```{r}
anti_join(their_bake,results)
```

```{r}

results =results |>
  mutate(baker_first_name = ifelse(baker_first_name == "Joanne", "Jo", baker_first_name))

```

Then I implemented the left join  between `their_bake` and `individual_bake` and matched on `series` and `baker_first_name`. After the left join, I performed a right join between the result of the first join and `results`. Then I also reordered the joining dataframe to make some important information comes first. And the table is further export into a csv in the directory containing the orginal dataset. The final dataset now reflects the personal information of the participants in the Great British Bake off, and the corresponding results in each season and episode.

```{r}
joining = their_bake|>
  left_join(individual_bake, by = c("series", "baker_first_name")) |>
  right_join(results, by = c("series", "baker_first_name", "episode"))|>
  select(series,episode,baker_first_name,baker_name,baker_age,result,everything())

```

```{r}
write.csv(joining , "./data/gbb_datasets/combined.csv", row.names = FALSE)
```

Then a user friendly table is created to show the start baker or the winner of each episode in Seasons 5 to 10. 
```{r}
user_friendly_table=joining|> 
  filter(series>=5 & series <= 10 & result %in% c("Star Baker", "Series Winner")) |>
  select(baker_name,series,episode,result)
  
  
```

```{r}
user_friendly_table
```

From the user-friendly table, it can be observed that the Series Winner is often one of the frequent Star Bakers. However, the competition can be unpredictable, and even those who win the most Star Baker titles aren't always guaranteed to win the series. As a result, the Series Winner may sometimes be someone who doesn't earn the most Star Baker titles throughout the season.

Import, clean and organize for viewers.csv
The steps for cleaning include figuring out the potential NA values and mutate the season variable to integer. Then the data is organized using `pivot_longer`.
```{r}
viewer=read_csv("data/gbb_datasets/viewers.csv",na = c("NA", ".", ""))|>
  janitor::clean_names() |>
  pivot_longer(
    series_1:series_10,
    names_to="season",
    names_prefix="series_",
    values_to="viewers"
  )|> mutate(season=as.integer(season))
head(viewer,10)


  
```


The average viewership in Season 1 is `r mean(viewer|>filter(season==1)|>pull(viewers),na.rm = TRUE)`.

The average viewership in Season 5 is `r round(mean(viewer|>filter(season==5)|> pull(viewers), na.rm = TRUE), 2)`.


