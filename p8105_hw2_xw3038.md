Homework2
================
Avery Wang
2024-10-01

## Problem 1

Load the necessary packages for this assignment

``` r
library(tidyverse)
library(dplyr)
library(readxl)
```

Read and clean the data; retain line, station, name, station latitude /
longitude, routes served, entry, vending, entrance type, and ADA
compliance. Convert the entry variable from character (YES vs NO) to a
logical variable (the ifelse or case_match function may be useful).

``` r
#load the data using relative path
NYC_subway=read_csv("./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv",
                    col_types = cols(Route8 =col_character(), 
                                     Route9 = col_character(), 
                                     Route10 =col_character(), 
                                     Route11 = col_character())) |>
  janitor::clean_names() |> 
  select(line:exit_only,vending,ada) |>
  mutate(entry = ifelse(entry == "YES", TRUE, FALSE))
head(NYC_subway,5)
## # A tibble: 5 × 20
##   line     station_name station_latitude station_longitude route1 route2 route3
##   <chr>    <chr>                   <dbl>             <dbl> <chr>  <chr>  <chr> 
## 1 4 Avenue 25th St                  40.7             -74.0 R      <NA>   <NA>  
## 2 4 Avenue 25th St                  40.7             -74.0 R      <NA>   <NA>  
## 3 4 Avenue 36th St                  40.7             -74.0 N      R      <NA>  
## 4 4 Avenue 36th St                  40.7             -74.0 N      R      <NA>  
## 5 4 Avenue 36th St                  40.7             -74.0 N      R      <NA>  
## # ℹ 13 more variables: route4 <chr>, route5 <chr>, route6 <chr>, route7 <chr>,
## #   route8 <chr>, route9 <chr>, route10 <chr>, route11 <chr>,
## #   entrance_type <chr>, entry <lgl>, exit_only <chr>, vending <chr>, ada <lgl>
```

The variables include: Line, station name, station latitude and
longtitude, route 1 to 11, entrance type, entry, exit_only and ada
information.

The first step is to convert the Route8 to 11 into character type so
that all the types of Route are now consistent. Then the
`janitor::clean_names()` is used to standardize the column names. And
specific columns according to the instruction are selected. Then the
entry variable is converted to the logical variable using ifelse. The
cleaned dataset now has 1868 rows and 20 columns. And these data are not
tidy as we should convert `route` variables from wide to long format.

``` r
filter_station=select(NYC_subway,line,station_name,ada,entry,vending)
```

- There are 465 distinct stations

- There are 84 stations that are ADA compliant

``` r
# Filter for stations without vending machines
without_vending=nrow( filter_station |>
  filter(vending == "NO") )

# Calculate the proportion of entrances without vending machines that allow entry
noentry_without_vending=nrow( filter_station |>
  filter(vending == "NO" & entry=="TRUE") )
```

\*The proportion of station entrances / exits without vending allow
entrance is 0.3770492

Reformat data so that route number and route name are distinct
variables.

``` r
reformat_df=
  NYC_subway |>
  mutate(across(starts_with("route"), as.character)) |>
  pivot_longer(
    route1:route11, 
    names_to = "route_number", 
    values_to = "route_name", 
    values_drop_na = TRUE
    )
```

``` r
A_station=reformat_df|>filter(route_name == "A")|>distinct(line,station_name)
A_train_ada= reformat_df|>filter(route_name == "A"& ada=="TRUE")|>distinct(line,station_name)
```

- There are 60 distinct stations served the A train.
- There are 17 distinct A stations that are ADA compliant

## Problem 2

Read and clean the Mr. Trash Wheel Sheet: Load the excel using
`read_excel` and specify the Mr. Trash Wheel sheet and figure out some
potential NA cases. Then the `janitor::clean_names()` is used to
standardize the column names. The rows without dumspter specific data
are dropped the sports ball column is converted to the integer variable
using `as.integer`. In order to differentiate the trash wheel type and
help prepare the steps for combining, a column named trash_wheel is also
added. And the year is converted to integer as well.

The sheet Professor trash wheel and Gwynndan trash wheel are loaded and
cleaned similarly

``` r
#load the Mr. Trash sheet by specifying the excel file
Mr_trash_wheel_sheet=read_excel("./data/Trash_Wheel_Collection_Data.xlsx",
                                sheet = "Mr. Trash Wheel", 
                                na = c("NA", ".", "")) |>
   janitor::clean_names() |> #give reasonable name 
   select(dumpster:homes_powered)|>
   drop_na(dumpster)|> #remove the rows without dumpster specific data
   mutate(sports_balls = as.integer(round(sports_balls)),
          trash_wheel= "Mr. Trash Wheel", 
          year = as.integer(year))

Professor_trash_wheel_sheet=read_excel("./data/Trash_Wheel_Collection_Data.xlsx",
                                sheet = "Professor Trash Wheel",
                                na = c("NA", ".", "")) |>
   janitor::clean_names() |> #give reasonable name 
   select(dumpster:homes_powered)|>
   drop_na(dumpster) |> #remove the rows without dumpster specific data
   mutate(trash_wheel ="Professor trash wheel",
           year = as.integer(year))

Gwynnda_trash_wheel=read_excel("./data/Trash_Wheel_Collection_Data.xlsx",
                                sheet = "Gwynnda Trash Wheel",
                                na = c("NA", ".", "")) |>
   janitor::clean_names() |> #give reasonable name 
   select(dumpster:homes_powered)|>
   drop_na(dumpster) |> #remove the rows without dumpster specific data
   mutate(trash_wheel ="Gwynnda trash wheel",
           year = as.integer(year))
```

- Mr. trash wheel sheet has 651 rows and 15 columns. With the variables:
  dumpster, month, year, date, weight_tons, volume_cubic_yards,
  plastic_bottles, polystyrene, cigarette_butts, glass_bottles,
  plastic_bags, wrappers, sports_balls, homes_powered, trash_wheel

- Professor trash wheel sheet has 119 rows and 14 columns. With the
  variables: dumpster, month, year, date, weight_tons,
  volume_cubic_yards, plastic_bottles, polystyrene, cigarette_butts,
  glass_bottles, plastic_bags, wrappers, homes_powered, trash_wheel

- Gwynnda trash wheel sheet has 263 rows and 13 columns. With the
  variables: dumpster, month, year, date, weight_tons,
  volume_cubic_yards, plastic_bottles, polystyrene, cigarette_butts,
  plastic_bags, wrappers, homes_powered, trash_wheel

Then combine the dataset with `bind_rows` :

``` r
# Combine all three datasets into one tidy dataset
combined_trash_wheel_data <- bind_rows(Mr_trash_wheel_sheet, Professor_trash_wheel_sheet, Gwynnda_trash_wheel)
head(combined_trash_wheel_data,5)
## # A tibble: 5 × 15
##   dumpster month  year date                weight_tons volume_cubic_yards
##      <dbl> <chr> <int> <dttm>                    <dbl>              <dbl>
## 1        1 May    2014 2014-05-16 00:00:00        4.31                 18
## 2        2 May    2014 2014-05-16 00:00:00        2.74                 13
## 3        3 May    2014 2014-05-16 00:00:00        3.45                 15
## 4        4 May    2014 2014-05-17 00:00:00        3.1                  15
## 5        5 May    2014 2014-05-17 00:00:00        4.06                 18
## # ℹ 9 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
## #   cigarette_butts <dbl>, glass_bottles <dbl>, plastic_bags <dbl>,
## #   wrappers <dbl>, sports_balls <int>, homes_powered <dbl>, trash_wheel <chr>
```

``` r
#filter by trash_wheel
weight_Professor=combined_trash_wheel_data |> 
  select(year,month,weight_tons,trash_wheel)|>
  filter(trash_wheel=="Professor trash wheel")

#filter by year and month for the Gwynnda trash wheel
cigg_gwy=combined_trash_wheel_data |> 
  select(year,month,cigarette_butts,trash_wheel)|>
  filter(trash_wheel=="Gwynnda trash wheel", year==2022, month=="June")
```

The new dataset has 1033 observations and 15 columns. With key
variables: dumpster, month, year, date, weight_tons, volume_cubic_yards,
plastic_bottles, polystyrene, cigarette_butts, glass_bottles,
plastic_bags, wrappers, sports_balls, homes_powered, trash_wheel.

- The total trash weight collected by Professor Trash Wheel is 246.74
  tons.

- The total number of cigarette butts collected by Gwynnda in June of
  2022 is 18120

## Problem 3

Import, clean and organize

``` r
individual_bake <- read_csv("./data/gbb_datasets/bakers.csv", na = c("NA", "N/A", "UNKNOWN", "Unknown", "")) |>
  janitor::clean_names() |>
  mutate(baker_first_name = sapply(strsplit(baker_name, " "), `[`, 1)) |> # Extract the first word as first name
  select(baker_first_name,everything())

their_bake= read_csv("./data/gbb_datasets/bakes.csv", na = c("NA", "N/A", "UNKNOWN", "Unknown", "")) |>
  janitor::clean_names() |>
  mutate(baker = ifelse(baker == "\"Jo\"", "Jo", baker)) |>
  select(baker,everything())|> #it is using the first name here 
  rename(baker_first_name=baker)

results = read_csv("./data/gbb_datasets/results.csv", skip=2)|> 
  janitor::clean_names() |> 
  select(baker, everything())|>
  rename(baker_first_name=baker)|>
  drop_na(result)|>
  mutate(
    result = case_match(
      result, 
      "IN" ~ "stayed in",
      "OUT" ~ "Eliminated",
      "STAR BAKER" ~ "Star Baker",
      "WINNER" ~ "Series Winner",
      "Runner-up" ~ "Series Runner up",
      "WD" ~ "withdrew"
    )
  )
```

For each dataset `bakers.csv, bakes.csv, and results.csv` the
`read_csv()` function reads the CSV files. Special values like “NA”,
“N/A”, “UNKNOWN”, “Unknown”, and empty strings are treated as missing
(NA). `janitor::clean_names()` is applied to each dataset to standardize
the columns.

In individual_bake, the `mutate()` function uses `strsplit()` and
`sapply()` to extract the first name from the baker_name column. This
helps prepare the joining step with their_bake as only first name of the
baker exist in that dataset. The first name is stored in a new column
baker_first_name.

In their_bake, the `mutate()` function is used to clean up
inconsistencies in the baker column. Specifically, if a baker’s name is
“"Jo"”, it is replaced with “Jo” to standardize the name. The baker
column is renamed to baker_first_name which helps the joining step with
the other two datasets.

In results dataframe, the first two columns are skipped since the
information starts from row 3. The column baker is also renamed to
baker_first_name, and the column result with missing values are dropped.
`case_match()` is also applied to clean and standardize values in the
result column.

To check for completeness and correctness across datasets, `anti_join`
is used. The anti_join of their_bake and individual_bake shows that all
the bakers in their_bake can find corresponding personal information in
the individual_bake dataframe.

``` r
anti_join(their_bake, individual_bake)
## # A tibble: 0 × 5
## # ℹ 5 variables: baker_first_name <chr>, series <dbl>, episode <dbl>,
## #   signature_bake <chr>, show_stopper <chr>
```

And the result of `anti_join(results,their_bake)` shows that in series
2, the person Jo cannot find the corresponding results in all the
episodes she attended. But since Joanne is the winner of series 2, Jo
actually might be Joanne. Correspondingly, I changed the first name
Joanne to Jo in the result dataframe

``` r
anti_join(their_bake,results)
## # A tibble: 8 × 5
##   baker_first_name series episode signature_bake                    show_stopper
##   <chr>             <dbl>   <dbl> <chr>                             <chr>       
## 1 Jo                    2       1 Chocolate Orange CupcakesOrange … Chocolate a…
## 2 Jo                    2       2 Caramelised Onion, Gruyere and T… Raspberry a…
## 3 Jo                    2       3 Stromboli flavored with Mozzarel… <NA>        
## 4 Jo                    2       4 Lavender Biscuits                 Blueberry M…
## 5 Jo                    2       5 Salmon and Asparagus Pie          Apple and R…
## 6 Jo                    2       6 Rum and Raisin Baked Cheesecake   Limoncello …
## 7 Jo                    2       7 Raspberry & Strawberry Mousse Ca… Pain Aux Ra…
## 8 Jo                    2       8 Raspberry and Blueberry Mille Fe… Mini Victor…
```

``` r

results =results |>
  mutate(baker_first_name = ifelse(baker_first_name == "Joanne", "Jo", baker_first_name))
```

Then I implemented the left join between `their_bake` and
`individual_bake` and matched on `series` and `baker_first_name`. After
the left join, I performed a right join between the result of the first
join and `results`. Then I also reordered the joining dataframe to make
some important information comes first. And the table is further export
into a csv in the directory containing the orginal dataset. The final
dataset now reflects the personal information of the participants in the
Great British Bake off, and the corresponding results in each season and
episode.

``` r
joining = their_bake|>
  left_join(individual_bake, by = c("series", "baker_first_name")) |>
  right_join(results, by = c("series", "baker_first_name", "episode"))|>
  select(series,episode,baker_first_name,baker_name,baker_age,result,everything())
```

``` r
write.csv(joining , "./data/gbb_datasets/combined.csv", row.names = FALSE)
```

Then a user friendly table is created to show the start baker or the
winner of each episode in Seasons 5 to 10.

``` r
user_friendly_table=joining|> 
  filter(series>=5 & series <= 10 & result %in% c("Star Baker", "Series Winner")) |>
  select(baker_name,series,episode,result)
  
  
```

``` r
user_friendly_table
## # A tibble: 60 × 4
##    baker_name        series episode result       
##    <chr>              <dbl>   <dbl> <chr>        
##  1 Nancy Birtwhistle      5       1 Star Baker   
##  2 Richard Burr           5       2 Star Baker   
##  3 Luis Troyano           5       3 Star Baker   
##  4 Richard Burr           5       4 Star Baker   
##  5 Kate Henry             5       5 Star Baker   
##  6 Chetna Makan           5       6 Star Baker   
##  7 Richard Burr           5       7 Star Baker   
##  8 Richard Burr           5       8 Star Baker   
##  9 Richard Burr           5       9 Star Baker   
## 10 Nancy Birtwhistle      5      10 Series Winner
## # ℹ 50 more rows
```

From the user-friendly table, it can be observed that the Series Winner
is often one of the frequent Star Bakers. However, the competition can
be unpredictable, and even those who win the most Star Baker titles
aren’t always guaranteed to win the series. As a result, the Series
Winner may sometimes be someone who doesn’t earn the most Star Baker
titles throughout the season.

Import, clean and organize for viewers.csv The steps for cleaning
include figuring out the potential NA values and mutate the season
variable to integer. Then the data is organized using `pivot_longer`.

``` r
viewer=read_csv("data/gbb_datasets/viewers.csv",na = c("NA", ".", ""))|>
  janitor::clean_names() |>
  pivot_longer(
    series_1:series_10,
    names_to="season",
    names_prefix="series_",
    values_to="viewers"
  )|> mutate(season=as.integer(season))
head(viewer,10)
## # A tibble: 10 × 3
##    episode season viewers
##      <dbl>  <int>   <dbl>
##  1       1      1    2.24
##  2       1      2    3.1 
##  3       1      3    3.85
##  4       1      4    6.6 
##  5       1      5    8.51
##  6       1      6   11.6 
##  7       1      7   13.6 
##  8       1      8    9.46
##  9       1      9    9.55
## 10       1     10    9.62
```

The average viewership in Season 1 is 2.77.

The average viewership in Season 5 is 10.04.
